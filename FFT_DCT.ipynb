{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a829a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb89da6cbbb4b8db1d3ad3e6d003224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 해상도: (480, 640)\n",
      "Saved: original_color.png, original_gray.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: recon_gray_fft_1.png, recon_color_fft_1.png\n",
      "Saved: recon_gray_fft_1_2.png, recon_color_fft_1_2.png\n",
      "Saved: recon_gray_fft_1_4.png, recon_color_fft_1_4.png\n",
      "Saved: recon_gray_fft_1_8.png, recon_color_fft_1_8.png\n",
      "Saved: recon_gray_fft_1_16.png, recon_color_fft_1_16.png\n",
      "Saved: recon_gray_dct_1.png, recon_color_dct_1.png\n",
      "Saved: recon_gray_dct_1_2.png, recon_color_dct_1_2.png\n",
      "Saved: recon_gray_dct_1_4.png, recon_color_dct_1_4.png\n",
      "Saved: recon_gray_dct_1_8.png, recon_color_dct_1_8.png\n",
      "Saved: recon_gray_dct_1_16.png, recon_color_dct_1_16.png\n",
      "\n",
      "=== FFT Reconstruction Metrics ===\n",
      "[480×640] GRAY → PSNR=322.443, NRMSE=0.000, SSIM=1.000, FID=-0.000 | COLOR → PSNR=323.275, NRMSE=0.000, SSIM=1.000, FID=0.001\n",
      "[240×320] GRAY → PSNR=16.925, NRMSE=0.502, SSIM=0.371, FID=432.233 | COLOR → PSNR=17.788, NRMSE=0.505, SSIM=0.375, FID=353.908\n",
      "[120×160] GRAY → PSNR=16.906, NRMSE=0.503, SSIM=0.367, FID=436.031 | COLOR → PSNR=17.769, NRMSE=0.506, SSIM=0.371, FID=356.581\n",
      "[60×80] GRAY → PSNR=16.871, NRMSE=0.505, SSIM=0.358, FID=430.092 | COLOR → PSNR=17.732, NRMSE=0.508, SSIM=0.363, FID=343.923\n",
      "[30×40] GRAY → PSNR=16.790, NRMSE=0.510, SSIM=0.350, FID=466.875 | COLOR → PSNR=17.646, NRMSE=0.513, SSIM=0.355, FID=399.903\n",
      "\n",
      "=== DCT Reconstruction Metrics ===\n",
      "[480×640] GRAY → PSNR=144.246, NRMSE=0.000, SSIM=1.000, FID=5.055 | COLOR → PSNR=145.234, NRMSE=0.000, SSIM=1.000, FID=3.334\n",
      "[240×320] GRAY → PSNR=35.933, NRMSE=0.056, SSIM=0.970, FID=3.967 | COLOR → PSNR=36.750, NRMSE=0.057, SSIM=0.972, FID=3.274\n",
      "[120×160] GRAY → PSNR=31.047, NRMSE=0.099, SSIM=0.913, FID=197.967 | COLOR → PSNR=31.816, NRMSE=0.100, SSIM=0.920, FID=145.313\n",
      "[60×80] GRAY → PSNR=27.363, NRMSE=0.151, SSIM=0.760, FID=348.864 | COLOR → PSNR=28.082, NRMSE=0.154, SSIM=0.777, FID=282.800\n",
      "[30×40] GRAY → PSNR=24.411, NRMSE=0.212, SSIM=0.612, FID=440.816 | COLOR → PSNR=25.077, NRMSE=0.218, SSIM=0.635, FID=377.621\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from scipy.fftpack import dct, idct\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import normalized_root_mse as nrmse\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import inception_v3\n",
    "from scipy.linalg import sqrtm\n",
    "from PIL import Image\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(\"Device:\", device)\n",
    "\n",
    "base = (\n",
    "    \"/Users/byeongchanmac/Library/CloudStorage/\"\n",
    "    \"GoogleDrive-jeong382@umn.edu/My Drive/\"\n",
    "    \"University of Minnesota/CSCI 5527/Project/\"\n",
    "    \"aloha_sim_insertion_human_image/data/chunk-000\"\n",
    ")\n",
    "ds = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\"train\": os.path.join(base, \"episode_*.parquet\")},\n",
    "    split=\"train\"\n",
    ")\n",
    "img_key = \"observation.images.top\"\n",
    "images = [\n",
    "    np.array(ex[img_key], np.float32) / 255.0\n",
    "    for ex in ds.select(range(min(100, len(ds))))\n",
    "]\n",
    "H, W = images[0].shape[:2]\n",
    "print(\"Original Resolution:\", (H, W))\n",
    "\n",
    "def fft_tr(x):\n",
    "    return np.fft.fft2(x)\n",
    "\n",
    "def fft_inv(f):\n",
    "    return np.abs(np.fft.ifft2(f))\n",
    "\n",
    "def dct_tr(x):\n",
    "    return dct(dct(x.T, norm=\"ortho\").T, norm=\"ortho\")\n",
    "\n",
    "def dct_inv(c):\n",
    "    return idct(idct(c.T, norm=\"ortho\").T, norm=\"ortho\")\n",
    "\n",
    "def reconstruct_gray(im, transform, inverse, bh, bw):\n",
    "    coeff = transform(im)\n",
    "    masked = np.zeros_like(coeff)\n",
    "    masked[:bh, :bw] = coeff[:bh, :bw]\n",
    "    return inverse(masked)\n",
    "\n",
    "def reconstruct_color(im_color, transform, inverse, bh, bw):\n",
    "    chans = []\n",
    "    for c in range(3):\n",
    "        rec_c = reconstruct_gray(im_color[..., c], transform, inverse, bh, bw)\n",
    "        chans.append(rec_c)\n",
    "    rec = np.stack(chans, axis=-1)\n",
    "    assert rec.shape == im_color.shape\n",
    "    return rec\n",
    "\n",
    "def eval_gray(origs, recs):\n",
    "    ps, nr, ss_vals = [], [], []\n",
    "    for o, r in zip(origs, recs):\n",
    "        dr = o.max() - o.min() or 1.0\n",
    "        ps.append(psnr(o, r, data_range=dr))\n",
    "        nr.append(nrmse(o, r))\n",
    "        ss_vals.append(ssim(o, r, data_range=dr))\n",
    "    return {\n",
    "        \"PSNR\": np.mean(ps),\n",
    "        \"NRMSE\": np.mean(nr),\n",
    "        \"SSIM\": np.mean(ss_vals)\n",
    "    }\n",
    "\n",
    "def eval_color(origs, recs):\n",
    "    ps, nr, ss_vals = [], [], []\n",
    "    for o, r in zip(origs, recs):\n",
    "        dr = o.max() - o.min() or 1.0\n",
    "        ps.append(psnr(o, r, data_range=dr))\n",
    "        nr.append(nrmse(o.flatten(), r.flatten()))\n",
    "        ss_vals.append(ssim(o, r, data_range=dr, channel_axis=-1))\n",
    "    return {\n",
    "        \"PSNR\": np.mean(ps),\n",
    "        \"NRMSE\": np.mean(nr),\n",
    "        \"SSIM\": np.mean(ss_vals)\n",
    "    }\n",
    "\n",
    "def get_acts(imgs, model, device, bs=32):\n",
    "    model.eval()\n",
    "    tf = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Resize((299, 299)),\n",
    "        T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "    acts = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(imgs), bs):\n",
    "            batch = imgs[i:i+bs]\n",
    "            tns = []\n",
    "            for im in batch:\n",
    "                im255 = (im * 255).clip(0,255).astype(np.uint8)\n",
    "                tns.append(tf(im255).unsqueeze(0))\n",
    "            tns = torch.cat(tns, 0).to(device)\n",
    "            acts.append(model(tns).cpu().numpy())\n",
    "    return np.concatenate(acts, 0)\n",
    "\n",
    "def compute_fid(origs, recs, device):\n",
    "    net = inception_v3(pretrained=True, transform_input=False)\n",
    "    net.fc = torch.nn.Identity()\n",
    "    net.to(device)\n",
    "    a1 = get_acts(origs, net, device)\n",
    "    a2 = get_acts(recs, net, device)\n",
    "    mu1, mu2 = a1.mean(0), a2.mean(0)\n",
    "    s1, s2 = np.cov(a1, rowvar=False), np.cov(a2, rowvar=False)\n",
    "    cs = sqrtm(s1.dot(s2))\n",
    "    if np.iscomplexobj(cs):\n",
    "        cs = cs.real\n",
    "    return np.sum((mu1 - mu2)**2) + np.trace(s1 + s2 - 2*cs)\n",
    "\n",
    "gray = [im.mean(-1) for im in images]\n",
    "\n",
    "out_dir = \"recons\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "scales = {\n",
    "    \"1\":    (H, W),\n",
    "    \"1/2\":  (H//2, W//2),\n",
    "    \"1/4\":  (H//4, W//4),\n",
    "    \"1/8\":  (H//8, W//8),\n",
    "    \"1/16\": (H//16, W//16)\n",
    "}\n",
    "methods = [(\"FFT\", (fft_tr, fft_inv)), (\"DCT\", (dct_tr, dct_inv))]\n",
    "\n",
    "sample_idx = 0\n",
    "orig_c = (images[sample_idx] * 255).clip(0,255).astype(np.uint8)\n",
    "Image.fromarray(orig_c).save(os.path.join(out_dir, \"original_color.png\"))\n",
    "orig_g = (gray[sample_idx] * 255).clip(0,255).astype(np.uint8)\n",
    "Image.fromarray(orig_g).save(os.path.join(out_dir, \"original_gray.png\"))\n",
    "print(\"Saved: original_color.png, original_gray.png\")\n",
    "\n",
    "results = {}\n",
    "for method, (tr, inv) in methods:\n",
    "    results[method] = {}\n",
    "    for name, (bh, bw) in scales.items():\n",
    "        recs_g = [reconstruct_gray(im, tr, inv, bh, bw) for im in gray]\n",
    "        recs_c = [reconstruct_color(im, tr, inv, bh, bw) for im in images]\n",
    "\n",
    "        m_gray  = eval_gray(gray, recs_g)\n",
    "        m_color = eval_color(images, recs_c)\n",
    "\n",
    "        orig_gray_rgb = [np.stack([img]*3, axis=-1) for img in gray]\n",
    "        rec_gray_rgb  = [np.stack([img]*3, axis=-1) for img in recs_g]\n",
    "        fid_gray  = compute_fid(orig_gray_rgb, rec_gray_rgb, device)\n",
    "        fid_color = compute_fid(images, recs_c, device)\n",
    "\n",
    "        results[method][name] = {\n",
    "            \"gray\": m_gray,\n",
    "            \"color\": m_color,\n",
    "            \"FID_gray\": fid_gray,\n",
    "            \"FID_color\": fid_color\n",
    "        }\n",
    "\n",
    "        safe = name.replace(\"/\", \"_\")\n",
    "        \n",
    "        g_arr = (recs_g[sample_idx] * 255).clip(0,255).astype(np.uint8)\n",
    "        Image.fromarray(g_arr).save(\n",
    "            os.path.join(out_dir, f\"recon_gray_{method.lower()}_{safe}.png\")\n",
    "        )\n",
    "        c_arr = (recs_c[sample_idx] * 255).clip(0,255).astype(np.uint8)\n",
    "        Image.fromarray(c_arr).save(\n",
    "            os.path.join(out_dir, f\"recon_color_{method.lower()}_{safe}.png\")\n",
    "        )\n",
    "        print(f\"Saved: recon_gray_{method.lower()}_{safe}.png, recon_color_{method.lower()}_{safe}.png\")\n",
    "\n",
    "label_map = {\n",
    "    \"1\": \"480×640\", \"1/2\": \"240×320\",\n",
    "    \"1/4\": \"120×160\", \"1/8\": \"60×80\",\n",
    "    \"1/16\": \"30×40\"\n",
    "}\n",
    "for method in results:\n",
    "    print(f\"\\n=== {method} Reconstruction Metrics ===\")\n",
    "    for name, m in results[method].items():\n",
    "        size = label_map[name]\n",
    "        print(\n",
    "            f\"[{size}] \"\n",
    "            f\"GRAY → PSNR={m['gray']['PSNR']:.3f}, \"\n",
    "            f\"NRMSE={m['gray']['NRMSE']:.3f}, \"\n",
    "            f\"SSIM={m['gray']['SSIM']:.3f}, \"\n",
    "            f\"FID={m['FID_gray']:.3f} | \"\n",
    "            f\"COLOR → PSNR={m['color']['PSNR']:.3f}, \"\n",
    "            f\"NRMSE={m['color']['NRMSE']:.3f}, \"\n",
    "            f\"SSIM={m['color']['SSIM']:.3f}, \"\n",
    "            f\"FID={m['FID_color']:.3f}\"\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
