{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b4de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os, time\n",
    "from datetime import datetime\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim_metric\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr_metric\n",
    "from skimage.metrics import normalized_root_mse as nrmse_metric\n",
    "from torchvision.utils import save_image\n",
    "from pytorch_fid import fid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_checkpoint(checkpoint_dir, model, optimizer):\n",
    "    \"\"\"\n",
    "    가장 최근 수정된 체크포인트(.pth 파일)를 자동으로 찾아서 불러오는 함수입니다.\n",
    "\n",
    "    Parameters:\n",
    "    - checkpoint_dir (str): 체크포인트가 저장된 디렉토리 경로\n",
    "    - model (torch.nn.Module): 학습할 모델 객체\n",
    "    - optimizer (torch.optim.Optimizer): 옵티마이저 객체\n",
    "\n",
    "    Returns:\n",
    "    - int: 저장된 epoch (없으면 0 반환)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. 디렉토리 내의 .pth 파일 목록을 가져옴\n",
    "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
    "\n",
    "    # 2. 체크포인트가 하나도 없으면 경고 출력 후 epoch 0 반환\n",
    "    if not checkpoint_files:\n",
    "        print(\"No Check Point Files. Start New Training.\")\n",
    "        return 0\n",
    "\n",
    "    # 3. 가장 최근에 수정된 파일을 찾음\n",
    "    checkpoint_files.sort(key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)), reverse=True)\n",
    "    latest_checkpoint = os.path.join(checkpoint_dir, checkpoint_files[0])\n",
    "    print(f\"The Latest Check Point: {latest_checkpoint}\")\n",
    "\n",
    "    # 4. 체크포인트 파일 로드\n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "\n",
    "    # 5. 모델과 옵티마이저에 state_dict 적용\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # 6. 저장된 epoch 반환\n",
    "    return checkpoint.get('epoch', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680aaa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf46109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed\n",
    "torch.manual_seed(316)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859dbdc1",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5460770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "device = get_device()\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ec7f96",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e1e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class AlohaImageDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, camera_key=\"observation.images.top\"):\n",
    "        self.data = hf_dataset\n",
    "        self.camera_key = camera_key\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx][self.camera_key]\n",
    "        image = self.transform(image)\n",
    "        return image, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28945589",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"lerobot/aloha_sim_insertion_human_image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a63eb3",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d1ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = AlohaImageDataset(dataset[\"train\"])\n",
    "\n",
    "len_trainset = int(len(full_dataset) * 0.8)\n",
    "len_valset = len(full_dataset) - len_trainset\n",
    "\n",
    "trainset, valset = random_split(full_dataset, [len_trainset, len_valset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716584f7",
   "metadata": {},
   "source": [
    "### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e78a7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader for the training set\n",
    "batch_size = 1\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e4486",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f5763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader for the validation set\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "    valset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e69308",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, _ = next(iter(trainloader))\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08547ba",
   "metadata": {},
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc99deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder\n",
    "class Autoencoder4x(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder4x, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=3, padding=1),  # 1 → 4채널\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # 해상도 2배 축소\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4, 1, kernel_size=2, stride=2),  # 원래 해상도 복원\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "\n",
    "model = Autoencoder4x().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac98c679",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e91dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(inputs, noise_factor=0.3):\n",
    "    noised = inputs + torch.randn_like(inputs) * noise_factor\n",
    "    noised = torch.clip(noised, 0.0, 1.0)\n",
    "    return noised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267e55c",
   "metadata": {},
   "source": [
    "### Check point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f254e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"CNN\"\n",
    "n_epochs = 20\n",
    "# CKPT_DIR = f\"ckpt/{DATASET}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{n_epochs}\"\n",
    "CKPT_DIR = f\"ckpt/{DATASET}\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CKPT_DIR, \"autoencoder.pth\")\n",
    "\n",
    "start_epoch = 0\n",
    "if os.path.exists(ckpt_path):\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "else:\n",
    "    print(\"Starting new training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb2a58f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab72df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in tqdm(range(start_epoch, n_epochs)):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for images, _ in trainloader:\n",
    "        images = images.to(device)\n",
    "        # image_noised = add_noise(images)\n",
    "        output, _ = model(images)\n",
    "        # denoised, _ = model(image_noised)\n",
    "        loss = criterion(output, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(trainloader)\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, _ in valloader:\n",
    "            images = images.to(device)\n",
    "            # image_noised = add_noise(images)\n",
    "            output, _ = model(images)\n",
    "            loss = criterion(output, images)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(valloader)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}\")\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, ckpt_path)\n",
    "\n",
    "print(f\"Training complete in {time.time() - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818d6c1a",
   "metadata": {},
   "source": [
    "## Meterics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.plot(train_loss_history, label=\"Train Loss\")\n",
    "\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_title(\"Train Loss\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547a803",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.plot(train_loss_history, label=\"Train Loss\")\n",
    "ax.plot(val_loss_history, label=\"Validation Loss\")\n",
    "\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_title(\"Train Loss & Validation Loss\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a64df6",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed9fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_img = 6\n",
    "\n",
    "# loader for the testset\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    valset,\n",
    "    batch_size=n_test_img,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e4ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_images, _ = next(iter(testloader))  # 배치에서 일부 가져오기\n",
    "    # sample_images = sample_images.to(device)\n",
    "    # images_noised = add_noise(sample_images)\n",
    "    sample_images = sample_images[:8].to(device)\n",
    "    reconstructed, latent = model(sample_images)\n",
    "\n",
    "# 압축된 latent space를 480x640으로 업샘플링 (4배 압축 대비)\n",
    "latent_vis = F.interpolate(latent, size=(480, 640), mode='bilinear', align_corners=False)\n",
    "\n",
    "# 평가 지표 계산\n",
    "h, w = 120, 160\n",
    "FID_REAL = \"fid_temp/real_120x160\"\n",
    "FID_FAKE = \"fid_temp/fake_120x160\"\n",
    "os.makedirs(FID_REAL, exist_ok=True)\n",
    "os.makedirs(FID_FAKE, exist_ok=True)\n",
    "\n",
    "psnr_list, ssim_list, nrmse_list = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, _) in enumerate(testloader):\n",
    "        images = images.to(device)\n",
    "        # noised = add_noise(images) \n",
    "        decoded, _ = model(images)\n",
    "\n",
    "        resized_original = F.interpolate(images, size=(h, w), mode='bilinear', align_corners=False)\n",
    "        resized_decoded = F.interpolate(decoded, size=(h, w), mode='bilinear', align_corners=False)\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            save_image(resized_original[i], f\"{FID_REAL}/{batch_idx}_{i}.png\")\n",
    "            save_image(resized_decoded[i], f\"{FID_FAKE}/{batch_idx}_{i}.png\")\n",
    "\n",
    "        original_np = resized_original.cpu().numpy()\n",
    "        decoded_np = resized_decoded.cpu().numpy()\n",
    "        for i in range(images.size(0)):\n",
    "            psnr_list.append(psnr_metric(original_np[i, 0], decoded_np[i, 0], data_range=1.0))\n",
    "            ssim_list.append(ssim_metric(original_np[i, 0], decoded_np[i, 0], data_range=1.0))\n",
    "            nrmse_list.append(nrmse_metric(original_np[i, 0], decoded_np[i, 0]))\n",
    "\n",
    "fid_val = fid_score.calculate_fid_given_paths([FID_REAL, FID_FAKE], batch_size=32, device=device, dims=2048)\n",
    "print(\"=== Autoencoder Reconstruction Metrics ===\")\n",
    "print(f\"[{h}×{w}] PSNR={np.mean(psnr_list):.3f}, NRMSE={np.mean(nrmse_list):.3f}, \"\n",
    "      f\"SSIM={np.mean(ssim_list):.3f}, FID={fid_val:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e11b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. 원본 이미지\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# plt.imshow(images[0].cpu().squeeze(0), cmap=\"gray\")\n",
    "# plt.title(\"Original\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# # 2. 압축 이미지 (latent 평균)\n",
    "# compressed_image = latent_vis[0].cpu().mean(0)\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# plt.imshow(compressed_image, cmap=\"gray\")\n",
    "# plt.title(\"2x Compressed\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# # 3. 복원 이미지\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# plt.imshow(denoised[0].cpu().squeeze(0), cmap=\"gray\")\n",
    "# plt.title(\"Reconstructed\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# 1. 원본 이미지\n",
    "grid = make_grid(sample_images.cpu(), nrow=8, padding=2)\n",
    "plt.figure(figsize=(12, 2))\n",
    "# plt.imshow(images[0].cpu().squeeze(0), cmap=\"gray\")\n",
    "plt.title(\"Original Images\")\n",
    "plt.imshow(grid.permute(1, 2, 0).squeeze(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# 2. 압축 이미지 (latent 평균)\n",
    "latent_mean = latent.mean(dim=1, keepdim=True)  # (B, 1, H, W)\n",
    "latent_norm = (latent_mean - latent_mean.min()) / (latent_mean.max() - latent_mean.min() + 1e-8)\n",
    "latent_vis = F.interpolate(latent_mean, size=(480, 640), mode='bilinear', align_corners=False)\n",
    "\n",
    "grid_latent = make_grid(latent_vis.cpu(), nrow=8, padding=2)\n",
    "plt.figure(figsize=(12, 2))\n",
    "# plt.imshow(compressed_image, cmap=\"gray\")\n",
    "plt.title(\"Compressed Latent Images (Upsampled to 480x640)\")\n",
    "plt.imshow(grid_latent.permute(1, 2, 0).squeeze(), cmap=\"gray\", vmin=0, vmax=1)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# 3. 복원 이미지\n",
    "grid_reconstructed = make_grid(reconstructed.cpu(), nrow=8, padding=2)\n",
    "plt.figure(figsize=(12, 2))\n",
    "# plt.imshow(denoised[0].cpu().squeeze(0), cmap=\"gray\")\n",
    "plt.title(\"Reconstructed Images\")\n",
    "plt.imshow(grid_reconstructed.permute(1, 2, 0).squeeze(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f414168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fvcore.nn import FlopCountAnalysis\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "model.eval()\n",
    "\n",
    "macs, params = get_model_complexity_info(\n",
    "    model, \n",
    "    (1, 480, 640), \n",
    "    as_strings=True,\n",
    "    print_per_layer_stat=False\n",
    ")\n",
    "\n",
    "print(f\"[ptflops] MACs (FLOPs): {macs}\")\n",
    "print(f\"[ptflops] Parameters: {params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
